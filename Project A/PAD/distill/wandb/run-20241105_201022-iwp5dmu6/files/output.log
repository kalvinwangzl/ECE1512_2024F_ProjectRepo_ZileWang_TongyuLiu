Distributed training:  False
Hyper-parameters:
 {'cfg': '../configs/mnist_IPC50.yaml', 'dataset': 'MNIST', 'subset': 'imagenette', 'model': 'ConvNet', 'ipc': 50, 'eval_mode': 'S', 'num_eval': 1, 'eval_it': 500, 'epoch_eval_train': 1000, 'Iteration': 10000, 'lr_img': 1000, 'lr_teacher': 0.01, 'lr_init': 0.01, 'batch_real': 256, 'batch_syn': 1000, 'batch_train': 128, 'pix_init': 'samples_predicted_correctly', 'dsa': True, 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'data_path': '../dataset', 'buffer_path': '../buffer_storage/', 'expert_epochs': 3, 'syn_steps': 80, 'max_start_epoch': 4, 'min_start_epoch': 0, 'zca': True, 'load_all': False, 'no_aug': False, 'texture': False, 'canvas_size': 2, 'canvas_samples': 1, 'max_files': None, 'max_experts': None, 'force_save': False, 'ema_decay': 0.9995, 'lr_y': 2.0, 'Momentum_y': 0.9, 'project': 'CIFAR10_ipc50', 'name': 'RANDOM', 'threshold': 1.0, 'loss_ratio': 0.25, 'depth_ratio': 0.25, 'record_loss': False, 'Sequential_Generation': True, 'expansion_end_epoch': 2000, 'current_max_start_epoch': 2, 'init_frozen': 'start', 'skip_first_eva': False, 'parall_eva': False, 'lr_lr': 1e-05, 'res': 32, 'device': 'cuda', 'Initialize_Label_With_Another_Model': False, 'Initialize_Label_Model': '', 'Initialize_Label_Model_Dir': '', 'Label_Model_Timestamp': -1, 'zca_trans': ZCAWhitening(), 'im_size': [28, 28], 'dc_aug_param': None, 'dsa_param': <utils.utils_baseline.ParamDiffAug object at 0x7cd24af77af0>, '_wandb': {}, 'distributed': False}
Evaluation model pool:  ['ConvNet']
BUILDING DATASET
  0% 0/60000 [00:00<?, ?it/s]/content/drive/MyDrive/Colab Notebooks/projecta/PAD/PAD/distill/PAD_depth.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels_all.append(class_map[torch.tensor(sample[1]).item()])
100% 60000/60000 [00:01<00:00, 53010.86it/s]
60000it [00:00, 733778.79it/s]
class c = 0: 5923 real images
class c = 1: 6742 real images
class c = 2: 5958 real images
class c = 3: 6131 real images
class c = 4: 5842 real images
class c = 5: 5421 real images
class c = 6: 5918 real images
class c = 7: 6265 real images
class c = 8: 5851 real images
class c = 9: 5949 real images
real images channel 0, mean = 0.0000, std = 0.3602
/content/drive/MyDrive/Colab Notebooks/projecta/PAD/PAD/distill/PAD_depth.py:138: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)
  label_syn = torch.tensor([np.ones(args.ipc) * i for i in range(num_classes)], dtype=torch.long, requires_grad=False,
Expert Dir: ../buffer_storage/MNIST/ConvNet
loading file ../buffer_storage/MNIST/ConvNet/replay_buffer_7.pt
/content/drive/MyDrive/Colab Notebooks/projecta/PAD/PAD/distill/PAD_depth.py:178: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  buffer = torch.load(expert_files[expert_id[file_idx]])
/content/drive/MyDrive/Colab Notebooks/projecta/PAD/PAD/distill/PAD_depth.py:208: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  temp_params = torch.load(label_expert_files[0])[0][args.Label_Model_Timestamp]
0.0.0
1.0.0
2.0.0
3.0.0
4.0.0
5.0.0
6.0.0
7.0.0
8.0.0
9.0.0
[2024-11-05 20:10:25] training begins
/content/drive/MyDrive/Colab Notebooks/projecta/PAD/PAD/distill/PAD_depth.py:292: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  Temp_Buffer = torch.load(label_expert_files[i])
InitialAcc:1.0
clear here
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  12415 KiB | 276246 KiB |  87089 MiB |  87077 MiB |
|       from large pool |  12334 KiB | 275720 KiB |  86312 MiB |  86300 MiB |
|       from small pool |     80 KiB |   2040 KiB |    777 MiB |    777 MiB |
|---------------------------------------------------------------------------|
| Active memory         |  12415 KiB | 276246 KiB |  87089 MiB |  87077 MiB |
|       from large pool |  12334 KiB | 275720 KiB |  86312 MiB |  86300 MiB |
|       from small pool |     80 KiB |   2040 KiB |    777 MiB |    777 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |  12411 KiB | 276243 KiB |  87089 MiB |  87077 MiB |
|       from large pool |  12333 KiB | 275718 KiB |  86312 MiB |  86300 MiB |
|       from small pool |     78 KiB |   2038 KiB |    776 MiB |    776 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 153600 KiB | 417792 KiB | 417792 KiB | 264192 KiB |
|       from large pool | 151552 KiB | 413696 KiB | 413696 KiB | 262144 KiB |
|       from small pool |   2048 KiB |   4096 KiB |   4096 KiB |   2048 KiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 141185 KiB | 152297 KiB |  36034 MiB |  35896 MiB |
|       from large pool | 139217 KiB | 150264 KiB |  35254 MiB |  35118 MiB |
|       from small pool |   1967 KiB |   2639 KiB |    779 MiB |    778 MiB |
|---------------------------------------------------------------------------|
| Allocations           |       9    |      23    |    8622    |    8613    |
|       from large pool |       4    |       8    |    2515    |    2511    |
|       from small pool |       5    |      18    |    6107    |    6102    |
|---------------------------------------------------------------------------|
| Active allocs         |       9    |      23    |    8622    |    8613    |
|       from large pool |       4    |       8    |    2515    |    2511    |
|       from small pool |       5    |      18    |    6107    |    6102    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       3    |       6    |       6    |       3    |
|       from large pool |       2    |       4    |       4    |       2    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       7    |    1489    |    1483    |
|       from large pool |       4    |       5    |     736    |     732    |
|       from small pool |       2    |       4    |     753    |     751    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
DSA augmentation strategy:
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters:
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'ratio_noise': 0.05, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}
  0% 0/1001 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
100% 1001/1001 [00:23<00:00, 43.49it/s]
[2024-11-05 20:10:51] Evaluate_00: epoch = 1000 train time = 23 s train loss = 0.142027 train acc = 0.0080, test acc = 0.9742
Evaluate 1 random ConvNet, mean = 0.9842 std = 0.0000
-------------------------
INFO:numexpr.utils:NumExpr defaulting to 12 threads.
get here!
Traceback (most recent call last):
  File "/content/drive/MyDrive/Colab Notebooks/projecta/PAD/PAD/distill/PAD_depth.py", line 645, in <module>
    main(args)
  File "/content/drive/MyDrive/Colab Notebooks/projecta/PAD/PAD/distill/PAD_depth.py", line 570, in main
    grad = torch.autograd.grad(ce_loss, student_params[-1], create_graph=True)[0]
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py", line 496, in grad
    result = _engine_run_backward(
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 250.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 102.81 MiB is free. Process 198764 has 39.46 GiB memory in use. Of the allocated memory 38.63 GiB is allocated by PyTorch, and 334.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
